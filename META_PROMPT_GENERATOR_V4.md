# TDD in the AI Era: Spawn-Experiments System v4.2 - Mechanical Rabbit Edition

**Purpose**: Optimized 4-method racing framework with Method 3 as the "mechanical rabbit" baseline for competitive methodology improvement.

**Key Innovation**: Method 3 (Pure TDD) serves as consistent baseline that other methodologies race against, driving continuous optimization through competitive dynamics.

**Version 4.2 Enhancements**:
- Method 3 established as "mechanical rabbit" baseline (consistent 6-8min, ~200 lines)
- Method 4 Adaptive TDD as innovation leader (4m dev, 1M+ val/sec, strategic validation)
- Enhanced branch protocol with automated verification and demo generation
- Prediction accountability system reveals AI biases and improves calibration
- Competitive racing framework drives methodology evolution

---

## ðŸ”¬ Core Experimental Framework

### **Step 0: Baseline Specification Generation**

```
BASELINE SPECIFICATION PROTOCOL:

1. Generate reasonable scope specification for [APPLICATION_TYPE]
2. Present to experimenter for approval/editing
3. Use approved baseline as shared foundation for all methods
4. Optional: Create specification variants (minimal/standard/detailed)

SPECIFICATION FORMAT:
- Core functionality requirements
- Key constraints and boundaries
- Expected input/output behavior
- Essential edge cases to handle
- Explicit exclusions (what NOT to build)

EXPERIMENTER DECISION POINT:
- Approve as-is
- Edit scope up/down
- Create multiple baselines for comparison
```

### **Step 1: Pre-Experiment Predictions**

```
PRE-EXPERIMENT PREDICTION PROTOCOL:

After baseline specification approval, generate methodology predictions:

1. Create experiment folder: experiments/[EXPERIMENT_NUMBER]/
2. Generate PRE_EXPERIMENT_PREDICTIONS.md with:
   - Expected outcomes for each methodology
   - Predicted code volume (lines)
   - Predicted development time
   - Anticipated architectural approaches
   - Methodology winner prediction
   - Potential surprises or challenges

PREDICTION FORMAT:
- Method-specific outcome predictions
- Quantitative estimates (time, lines, complexity)
- Architectural approach expectations
- Overall methodology performance ranking
- Specific areas where methods might struggle/excel

RESEARCH VALUE:
- Captures AI's initial expectations before execution
- Enables prediction accuracy analysis
- Reveals AI biases about methodology effectiveness
- Creates accountability for methodology assumptions
```

### **Enhanced Branch Isolation Protocol** (Add to ALL prompts)

```
IMPORTANT: Create a dedicated branch for your experiment with standardized structure:

git checkout -b exp-[EXPERIMENT_NUMBER]-[METHOD_NAME]

Examples:
- git checkout -b exp-1504-immediate
- git checkout -b exp-1504-specification
- git checkout -b exp-1504-tdd
- git checkout -b exp-1504-adaptive-tdd

REQUIRED FILE STRUCTURE:
experiments/[EXPERIMENT_NUMBER]/[METHOD_DIR]/
â”œâ”€â”€ [main_implementation_file] (place ALL files in method directory)
â”œâ”€â”€ test_*.py (if applicable)
â”œâ”€â”€ requirements.txt (if external libraries used)
â”œâ”€â”€ README.md (brief method summary)
â””â”€â”€ [other method-specific files]

CRITICAL: Keep experiments isolated in their designated directories:
- Place ALL implementation files in experiments/[EXPERIMENT_NUMBER]/[METHOD_DIR]/
- Do NOT place files in project root or other directories
- Use relative paths within your method directory
- Ensure clean separation from other methods

INTEGRATION VERIFICATION:
Before marking complete, verify your branch contains:
- Main implementation file in correct experiments/[NUMBER]/[METHOD]/ directory
- All files properly placed within your method directory (not project root)
- All required dependencies resolved (requirements.txt if external libraries)
- Clean commit history with descriptive messages
- No references to other methodology implementations
- No files accidentally placed outside your method directory

This ensures:
- Perfect methodology isolation during development
- Clean integration back to main branch
- Consistent file structure for comparison scripts
- Audit trail for scientific rigor
```

### **Python Environment Setup Protocol** (Add to ALL Python prompts)

```
IMPORTANT: Use standard venv for Python environment isolation:

SETUP:
1. Create virtual environment:
   python3 -m venv .venv

2. Activate environment:
   source .venv/bin/activate  # Linux/Mac
   .venv\Scripts\activate     # Windows

3. Install dependencies:
   pip install -r requirements.txt

REQUIREMENTS FILE:
- Create requirements.txt with pinned versions
- Include only necessary dependencies
- Example:
  pytest==7.4.3
  ollama==0.1.6

WHY venv NOT uv:
- Universal compatibility - no extra tools to install
- Standard Python tooling everyone knows
- Simpler for public repos and contributors
- Clear, well-understood dependency management

IMPORTANT:
- Do NOT use pip install without venv activated
- Do NOT use uv, poetry, or other non-standard tools
- Keep dependencies minimal and well-documented
```

### **Atomic Commit Protocol** (Add to ALL prompts)

```
IMPORTANT: Checkpoint your progress with atomic git commits:

After each meaningful step, commit your work:
- git add -A && git commit -m "Specs: [what you defined]"
- git add -A && git commit -m "Impl: [what you built]"
- git add -A && git commit -m "Test: [what you tested]"
- git add -A && git commit -m "Fix: [what you fixed]"
- git add -A && git commit -m "COMPLETE: [solution summary]"

Commit at least every 3 minutes or at major milestones.
This enables progress tracking and clean integration back to main.
```

### **Enhanced Integration Protocol**

```
EXPERIMENT COMPLETION WITH VERIFICATION:

1. PRE-INTEGRATION VERIFICATION on your branch:
   - Verify main implementation file exists and works
   - Run basic functionality test
   - Confirm no cross-method contamination
   - Clean commit history with atomic commits

2. INTEGRATION TO MAIN:
   git checkout main
   git merge --no-ff exp-[EXPERIMENT_NUMBER]-[METHOD_NAME]

   If conflicts occur:
   - DO NOT resolve by copying from other methods
   - Ensure your method's files take precedence in your directory
   - Verify post-merge that your implementation is intact

3. POST-INTEGRATION VERIFICATION:
   - Test that your method's implementation works on main
   - Verify comparison scripts can find your files
   - Run methodology_comparison_demo.py to confirm inclusion

4. BRANCH CLEANUP (optional):
   git branch -d exp-[EXPERIMENT_NUMBER]-[METHOD_NAME]

This ensures:
- No method implementations lost during integration
- Comparison scripts work with all methods
- Scientific integrity maintained through merge process
```

---

## ðŸ”§ Integration Tooling and Verification

### **Pre-Integration Verification Script**

```bash
#!/bin/bash
# verify_experiment_branch.sh - Run before merging to main

EXPERIMENT_NUM=$1
METHOD_NAME=$2
BRANCH_NAME="exp-${EXPERIMENT_NUM}-${METHOD_NAME}"

echo "ðŸ” Verifying experiment branch: $BRANCH_NAME"

# Check we're on the right branch
current_branch=$(git branch --show-current)
if [ "$current_branch" != "$BRANCH_NAME" ]; then
    echo "âŒ Not on branch $BRANCH_NAME (currently on $current_branch)"
    exit 1
fi

# Check required files exist
EXPERIMENT_DIR="experiments/$EXPERIMENT_NUM"
METHOD_DIR_MAP=("immediate:1-immediate-implementation" "specification:2-specification-driven" "tdd:3-test-first-development" "adaptive-tdd:4-adaptive-tdd")

for mapping in "${METHOD_DIR_MAP[@]}"; do
    method="${mapping%%:*}"
    dir="${mapping#*:}"
    if [[ "$METHOD_NAME" == *"$method"* ]]; then
        IMPL_FILE="$EXPERIMENT_DIR/$dir/date_validator.py"
        if [ ! -f "$IMPL_FILE" ]; then
            echo "âŒ Missing implementation file: $IMPL_FILE"
            exit 1
        fi
        echo "âœ… Found implementation: $IMPL_FILE"

        # Test the implementation works
        if python -c "
import sys; sys.path.insert(0, '$EXPERIMENT_DIR/$dir')
import date_validator
result = date_validator.validate_date('02/29/2024')
print('âœ… Implementation test passed:', result)
" 2>/dev/null; then
            echo "âœ… Implementation functional test passed"
        else
            echo "âŒ Implementation functional test failed"
            exit 1
        fi
        break
    fi
done

echo "ðŸŽ¯ Branch verification complete - ready for integration!"
```

### **Post-Integration Verification Script**

```bash
#!/bin/bash
# verify_main_integration.sh - Run after merging to main

EXPERIMENT_NUM=$1

echo "ðŸ” Verifying main branch integration for experiment $EXPERIMENT_NUM"

cd "experiments/$EXPERIMENT_NUM" || exit 1

# Test all method implementations
methods=("1-immediate-implementation" "2-specification-driven" "3-test-first-development" "4-adaptive-tdd")

for method_dir in "${methods[@]}"; do
    if [ -d "$method_dir" ] && [ -f "$method_dir/date_validator.py" ]; then
        echo "Testing $method_dir..."
        if python -c "
import sys; sys.path.insert(0, '$method_dir')
import date_validator
result = date_validator.validate_date('02/29/2024')
print('âœ… $method_dir: test passed')
" 2>/dev/null; then
            echo "âœ… $method_dir integration successful"
        else
            echo "âŒ $method_dir integration failed"
        fi
    else
        echo "âš ï¸  $method_dir not implemented"
    fi
done

# Test comparison script works
if [ -f "methodology_comparison_demo.py" ]; then
    echo "Testing comparison script..."
    if python methodology_comparison_demo.py >/dev/null 2>&1; then
        echo "âœ… Comparison script working"
    else
        echo "âŒ Comparison script failed"
    fi
fi

echo "ðŸŽ¯ Main branch integration verification complete!"
```

### **Automated Post-Experiment Demo Generation**

```bash
#!/bin/bash
# generate_experiment_demo.sh - Create comprehensive demo for completed experiment

EXPERIMENT_NUM=$1
EXPERIMENT_NAME=$2

echo "ðŸš€ Generating comprehensive demo for experiment $EXPERIMENT_NUM"

EXPERIMENT_DIR="experiments/$EXPERIMENT_NUM"
cd "$EXPERIMENT_DIR" || exit 1

# Auto-detect implemented methods
methods=()
method_names=()
if [ -f "1-immediate-implementation/date_validator.py" ]; then
    methods+=("1-immediate-implementation")
    method_names+=("Method 1 (Immediate)")
fi
if [ -f "2-specification-driven/date_validator.py" ]; then
    methods+=("2-specification-driven")
    method_names+=("Method 2 (Specification)")
fi
if [ -f "3-test-first-development/date_validator.py" ]; then
    methods+=("3-test-first-development")
    method_names+=("Method 3 (Pure TDD)")
fi
if [ -f "4-adaptive-tdd/date_validator.py" ]; then
    methods+=("4-adaptive-tdd")
    method_names+=("Method 4 (Adaptive TDD)")
fi

echo "ðŸ“Š Detected ${#methods[@]} implemented methods: ${method_names[*]}"

# Generate comparison demo script
cat > methodology_comparison_demo.py << 'EOF'
#!/usr/bin/env python3
"""
Auto-generated Methodology Comparison Demo
Automatically tests all available method implementations.
"""

import os
import sys
import subprocess
from pathlib import Path

def find_experiment_directory():
    """Find the experiment directory from any location."""
    current_dir = os.getcwd()
    script_dir = os.path.dirname(os.path.abspath(__file__))

    # If running from the experiment directory
    if os.path.basename(script_dir).startswith("1."):
        return script_dir

    return script_dir

def get_available_methods(base_dir):
    """Auto-detect available method implementations."""
    methods = {}

    method_dirs = {
        "Method 1 (Immediate)": "1-immediate-implementation",
        "Method 2 (Specification)": "2-specification-driven",
        "Method 3 (Pure TDD)": "3-test-first-development",
        "Method 4 (Adaptive TDD)": "4-adaptive-tdd"
    }

    for method_name, method_dir in method_dirs.items():
        impl_file = os.path.join(base_dir, method_dir, "date_validator.py")
        if os.path.exists(impl_file):
            methods[method_name] = method_dir

    return methods

def test_functionality_equivalence(base_dir, available_methods):
    """Test that all available methods provide equivalent functionality."""
    print("\n" + "="*80)
    print("ðŸ§ª FUNCTIONALITY EQUIVALENCE TEST")
    print("="*80)
    print(f"Testing {len(available_methods)} available method implementations...")
    print()

    test_cases = [
        ("02/29/2024", "Valid leap year"),
        ("02/29/2023", "Invalid leap year"),
        ("13/01/2024", "EU format valid"),
        ("01/13/2024", "US format valid"),
        ("", "Empty string"),
        ("not-a-date", "Invalid format")
    ]

    # Create header
    header = f"{'Test Case':<15}"
    for method_name in available_methods.keys():
        method_abbrev = "M" + method_name.split()[1][1]  # M1, M2, M3, M4
        header += f" {method_abbrev:<6}"
    header += " Description"

    print(header)
    print("-" * len(header))

    for test_input, description in test_cases:
        display_input = test_input if test_input else "(empty)"
        results = {}

        for method_name, method_dir in available_methods.items():
            method_path = os.path.join(base_dir, method_dir)

            try:
                test_script = f'''
import sys
sys.path.insert(0, "{method_path}")

test_input = """{test_input}"""
try:
    import date_validator
    result = date_validator.validate_date(test_input)
    print("âœ“" if result else "âœ—")
except Exception as e:
    print("Error")
'''

                result = subprocess.run(
                    [sys.executable, "-c", test_script],
                    capture_output=True, text=True, timeout=5
                )

                if result.returncode == 0:
                    results[method_name] = result.stdout.strip()
                else:
                    results[method_name] = "Error"

            except Exception:
                results[method_name] = "N/A"

        # Print results row
        row = f"{display_input:<15}"
        for method_name in available_methods.keys():
            result = results.get(method_name, "N/A")
            row += f" {result:<6}"
        row += f" {description}"
        print(row)

    print(f"\nðŸ“Š Successfully tested {len(available_methods)} method implementations!")

def main():
    """Run comprehensive methodology comparison."""
    print("="*80)
    print("ðŸš€ AUTOMATED METHODOLOGY COMPARISON")
    print("="*80)

    base_dir = find_experiment_directory()
    print(f"ðŸ“‚ Experiment directory: {base_dir}")

    available_methods = get_available_methods(base_dir)

    if not available_methods:
        print("âŒ No method implementations found!")
        return

    print(f"âœ… Found {len(available_methods)} implemented methods:")
    for method_name, method_dir in available_methods.items():
        print(f"   â€¢ {method_name}: {method_dir}/")

    test_functionality_equivalence(base_dir, available_methods)

    print("\n" + "="*80)
    print("ðŸŽ¯ METHODOLOGY COMPARISON COMPLETE")
    print("="*80)
    print("âœ… All available methods tested successfully!")
    print("ðŸ“Š Comparison data ready for analysis")

if __name__ == "__main__":
    main()
EOF

# Test the generated demo
echo "ðŸ§ª Testing generated comparison demo..."
if python methodology_comparison_demo.py; then
    echo "âœ… Demo generation successful!"
else
    echo "âŒ Demo generation failed - manual intervention needed"
    exit 1
fi

echo "ðŸŽ¯ Post-experiment demo generation complete!"
```

### **Complete Post-Experiment Protocol**

```
EXPERIMENT COMPLETION WORKFLOW:

1. INDIVIDUAL METHOD COMPLETION:
   - Complete work on your method branch
   - Run pre-integration verification script
   - Merge to main with enhanced integration protocol

2. AUTOMATED DEMO GENERATION:
   bash generate_experiment_demo.sh [EXPERIMENT_NUM] [EXPERIMENT_NAME]

   This automatically:
   - Detects all implemented methods
   - Generates working comparison demo
   - Tests functionality equivalence
   - Validates all methods are accessible

3. EXPERIMENT REPORT ORGANIZATION:
   - Create reports/ folder for detailed analysis files
   - Move specialized reports: performance, development time, prediction analysis
   - Keep main EXPERIMENT_REPORT.md as executive summary
   - Organize structure:
     experiments/[NUM]/
     â”œâ”€â”€ EXPERIMENT_REPORT.md (executive summary)
     â”œâ”€â”€ PRE_EXPERIMENT_PREDICTIONS.md
     â”œâ”€â”€ reports/
     â”‚   â”œâ”€â”€ PERFORMANCE_ANALYSIS.md
     â”‚   â”œâ”€â”€ DEVELOPMENT_TIME_ANALYSIS.md
     â”‚   â”œâ”€â”€ PREDICTION_ANALYSIS.md
     â”‚   â””â”€â”€ V4_FRAMEWORK_COMPARISON.md (if applicable)
     â””â”€â”€ [method directories...]

4. FINAL VERIFICATION:
   - All method implementations working
   - Comparison demo runs successfully
   - Complete experiment report generated
   - Integration scripts validated

This ensures every experiment produces:
âœ… Working implementations for all attempted methods
âœ… Automated comparison and testing
âœ… Comprehensive analysis and insights
âœ… Clean integration without manual file copying

5. SYSTEMATIC DOCUMENTATION UPDATES:
   - Update findings/ documents with new patterns
   - Update FUTURE_EXPERIMENTS_ROADMAP.md with results
   - Update EXPERIMENT_INDEX.md with latest experiment
   - Update README.md if significant breakthroughs
   - Update any domain-specific analysis documents

   Framework Integration Updates:
   - Add experiment to FRAMEWORK_VERSION_LOG.md
   - Update methodology performance summaries
   - Document any framework enhancements discovered
   - Add to methodology selection guidance
```

---

## ðŸŽ¯ **Benchmark Framework: The Constraint-Driven Baseline**

### **ðŸ”§ The Natural Constraint Baseline**
**Method 3 (Pure TDD)** serves as the consistent baseline standard for post-experiment comparison:
- **Benchmark Performance**: 6-8 minutes development time
- **Benchmark Efficiency**: ~200 lines for input validation tasks
- **Benchmark Reliability**: 100% implementation success rate
- **Natural Mechanism**: Tests prevent over-engineering automatically

**Framework Objective**: Methods are measured against this consistent baseline to reveal their effectiveness patterns.

---

## ðŸ† **Optimized Method Prompts**

### **âš¡ Method 1: Immediate Implementation** (Speed-Focused)

```
Build a [APPLICATION_TYPE] using [TECH_STACK].

REQUIREMENTS (Baseline Specification):
[APPROVED_BASELINE_SPECIFICATION]

Start coding immediately with minimal planning. Focus on getting something working quickly.

CONSTRAINTS:
- You may not use the web for this project
- Use only standard library and built-in capabilities
- Work from your existing knowledge

DIRECTORY STRUCTURE:
Create your implementation in:
experiments/[EXPERIMENT_NUMBER]/1-immediate-implementation/

BRANCH ISOLATION:
git checkout -b exp-[EXPERIMENT_NUMBER]-immediate

ATOMIC COMMIT PROTOCOL:
Commit your progress frequently:
- git add -A && git commit -m "Initial: Starting implementation"
- git add -A && git commit -m "Core: Basic functionality working"
- git add -A && git commit -m "Feature: [specific feature added]"
- git add -A && git commit -m "COMPLETE: Working solution"

Commit every 2-3 minutes or when you complete any working functionality.

Technology: [TECH_STACK]
Show all work including commits.
```

```
Build a [APPLICATION_TYPE] using [TECH_STACK].

REQUIREMENTS (Baseline Specification):
[APPROVED_BASELINE_SPECIFICATION]

Start coding immediately with minimal planning. Focus on getting something working quickly.

CONSTRAINTS:
- You may not use the web for this project
- Use only standard library and built-in capabilities
- Work from your existing knowledge

DIRECTORY STRUCTURE:
Create your implementation in:
experiments/[EXPERIMENT_NUMBER]/1-immediate-implementation/

BRANCH ISOLATION:
git checkout -b exp-[EXPERIMENT_NUMBER]-immediate

ATOMIC COMMIT PROTOCOL:
Commit your progress frequently:
- git add -A && git commit -m "Initial: Starting implementation"
- git add -A && git commit -m "Core: Basic functionality working"
- git add -A && git commit -m "Feature: [specific feature added]"
- git add -A && git commit -m "COMPLETE: Working solution"

Commit every 2-3 minutes or when you complete any working functionality.

Technology: [TECH_STACK]
Show all work including commits.
```

### **ðŸ“š Method 2: Specification-Driven** (Enterprise-Focused)

```
Build a [APPLICATION_TYPE] using [TECH_STACK].

REQUIREMENTS (Baseline Specification):
[APPROVED_BASELINE_SPECIFICATION]

Follow a realistic specification-driven workflow with human oversight.

PROCESS:
1. Create comprehensive implementation specifications
2. [HUMAN REVIEW CHECKPOINT - specifications will be reviewed/edited]
3. Implement against approved specifications with quality discipline

CONSTRAINTS:
- You may not use the web for this project
- Use only standard library and built-in capabilities
- Work from your existing knowledge

DIRECTORY STRUCTURE:
Create your implementation in:
experiments/[EXPERIMENT_NUMBER]/2-specification-driven/

BRANCH ISOLATION:
git checkout -b exp-[EXPERIMENT_NUMBER]-specification

ATOMIC COMMIT PROTOCOL:
Track your progress with git commits:
- git add -A && git commit -m "Specs: Implementation approach documented"
- git add -A && git commit -m "Specs: Technical design complete"
- git add -A && git commit -m "Impl: Core structure created"
- git add -A && git commit -m "Impl: [Feature] implemented"
- git add -A && git commit -m "COMPLETE: All requirements met"

Technology: [TECH_STACK]
Show all work including commits.
```

### **ðŸ”§ Method 3: Test-First Development** (Constraint-Driven Baseline)

```
Build a [APPLICATION_TYPE] using [TECH_STACK] using strict TDD.

REQUIREMENTS (Baseline Specification):
[APPROVED_BASELINE_SPECIFICATION]

Follow Red-Green-Refactor with atomic commits - natural constraint mechanism:

CONSTRAINTS:
- You may not use the web for this project
- Use only standard library and built-in capabilities
- Work from your existing knowledge

DIRECTORY STRUCTURE:
Create your implementation in:
experiments/[EXPERIMENT_NUMBER]/3-test-first-development/

BRANCH ISOLATION:
git checkout -b exp-[EXPERIMENT_NUMBER]-tdd

ATOMIC COMMIT PROTOCOL:
- git add -A && git commit -m "Test: [feature] - RED"  (after writing failing test)
- git add -A && git commit -m "Impl: [feature] - GREEN" (after making test pass)
- git add -A && git commit -m "Refactor: [improvement]" (if refactoring)
- git add -A && git commit -m "COMPLETE: All tests passing"

Each test cycle should be a separate commit.
This creates a clear TDD history.

Technology: [TECH_STACK]
Show all work including commits.
```

### **ðŸ† Method 4: Adaptive/Validated TDD** (Test Quality Verification)

```
Build a [APPLICATION_TYPE] using [TECH_STACK].

REQUIREMENTS (Baseline Specification):
[APPROVED_BASELINE_SPECIFICATION]

Use FULL TDD (test everything) with ADAPTIVE VALIDATION (verify test quality strategically):

âš ï¸ CRITICAL: This is NOT selective testing - you must test ALL code.
The "adaptive" part is about VALIDATING your tests, not skipping them.

TDD PROCESS (applies to ALL code):
1. RED: Write failing test
2. VALIDATE (adaptive - see below): Verify test catches bugs
3. GREEN: Write correct implementation
4. REFACTOR: Clean up

ADAPTIVE VALIDATION (the key innovation):
Apply extra validation step when you encounter:
- Complex edge cases that could be implemented incorrectly
- Non-obvious business logic that needs verification
- Areas where a wrong implementation might still pass naive tests
- Critical functionality where test quality matters most

VALIDATION TECHNIQUE:
1. After writing test (RED phase)
2. Write intentionally buggy implementation
3. Run test - it MUST fail
4. If test passes buggy code â†’ test is inadequate, fix test
5. Once test fails buggy code â†’ write correct implementation

EXAMPLE:
```python
# Step 1: Write test (RED)
def test_leap_year_validation():
    assert is_leap_year(2024) == True
    assert is_leap_year(2023) == False

# Step 2: VALIDATE test (for complex logic)
def is_leap_year(year):
    return True  # Intentionally wrong!

# Run test â†’ should FAIL
# If test passes, your test is inadequate

# Step 3: Write correct implementation (GREEN)
def is_leap_year(year):
    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)
```

WHEN TO VALIDATE:
- Complex algorithms â†’ YES (validate test quality)
- Business logic â†’ YES (validate test quality)
- Error handling â†’ YES (validate test quality)
- Simple assignment â†’ NO (standard TDD sufficient)
- Trivial string ops â†’ NO (standard TDD sufficient)

IMPORTANT:
- You MUST write tests for ALL code (this is TDD)
- You SELECTIVELY validate test quality (this is adaptive)
- Document validation decisions in commit messages

CONSTRAINTS:
- You may not use the web for this project
- Use only standard library and built-in capabilities
- Work from your existing knowledge

DIRECTORY STRUCTURE:
Create your implementation in:
experiments/[EXPERIMENT_NUMBER]/4-adaptive-tdd/

BRANCH ISOLATION:
git checkout -b exp-[EXPERIMENT_NUMBER]-adaptive-tdd

ATOMIC COMMIT PROTOCOL:
- git add -A && git commit -m "Plan: Requirements analysis complete"
- git add -A && git commit -m "Test: [feature] test written"
- git add -A && git commit -m "Impl: [feature] implementation"
- git add -A && git commit -m "Validation: [area] robustness verified" (when applicable)
- git add -A && git commit -m "COMPLETE: All requirements implemented"

Strategic efficiency through adaptive complexity matching.

Technology: [TECH_STACK]
Show all work including commits.
```

---

## ðŸš€ Baseline Specification Examples

### **Example: Date Format Validator**

```
GENERATED BASELINE SPECIFICATION:

Core Functionality:
- Accept MM/DD/YYYY and DD/MM/YYYY formats
- Validate date logic (no Feb 30, handle leap years correctly)
- Return boolean valid/invalid result
- Handle basic edge cases (empty strings, malformed input)

Constraints:
- US and European formats only (no other international formats)
- No timezone support required
- No date arithmetic or manipulation
- No date parsing beyond format validation

Expected Behavior:
- "02/29/2024" â†’ valid (leap year)
- "02/29/2023" â†’ invalid (not leap year)
- "13/01/2024" â†’ invalid in MM/DD, valid in DD/MM
- "" â†’ invalid
- "not-a-date" â†’ invalid

Exclusions:
- No internationalization beyond US/EU
- No advanced date libraries unless standard library
- No calendar system complexities
- No performance optimization for large datasets

EXPERIMENTER: [Approve/Edit this scope before proceeding]
```

### **Example: Phone Number Validator**

```
GENERATED BASELINE SPECIFICATION:

Core Functionality:
- Validate US phone number formats
- Accept (555) 123-4567, 555-123-4567, 5551234567 formats
- Return boolean valid/invalid result
- Basic format and length validation

Constraints:
- US numbers only (no international)
- No phone number lookup or carrier validation
- No extension support
- No formatting/normalization output

Expected Behavior:
- "(555) 123-4567" â†’ valid
- "555-123-4567" â†’ valid
- "5551234567" â†’ valid
- "555-123-456" â†’ invalid (too short)
- "555-123-45678" â†’ invalid (too long)

Exclusions:
- No international number support
- No real phone number database validation
- No area code validity checking
- No extension parsing

EXPERIMENTER: [Approve/Edit this scope before proceeding]
```

---

## ðŸ“Š Experimental Variations

### **Baseline Specification Variants**

**Minimal Baseline**: Core functionality only
- Essential requirements
- Basic constraints
- Minimal edge cases

**Standard Baseline**: Typical project scope (default)
- Complete functionality description
- Reasonable constraints
- Key edge cases covered

**Detailed Baseline**: Comprehensive requirements
- Extensive functionality specification
- Detailed constraints and exclusions
- Comprehensive edge case coverage

### **Research Questions**
1. How do methodologies perform with identical scope?
2. Which approach handles ambiguity most effectively?
3. What's the optimal specification detail level?
4. How does baseline scope affect methodology choice?

---

## ðŸŽ¯ Integration with Existing Research

### **Preserving Scientific Rigor**
- **Controlled experiments**: Identical baselines eliminate interpretation variance
- **Quantitative metrics**: Code quality, time, test coverage comparisons
- **Reproducibility**: Baseline specifications enable exact replication

### **Building on Phase 1 Findings**
- **TDD constraint effectiveness**: Validated across domains
- **Specification-driven risks**: Controlled through baseline approval
- **Practical team workflows**: Focus on real-world applicability

---

## ðŸ’¡ Usage Protocol

### **Standard Experiment Flow**
1. **Generate baseline**: `spawn-experiments` + APPLICATION_TYPE
2. **Review/approve scope**: Experimenter edits generated baseline
3. **Generate predictions**: AI predicts methodology outcomes before execution
4. **Launch parallel execution**: Four methods with identical baseline
5. **Analyze methodology effectiveness**: Compare actual vs predicted results
6. **Clean integration**: Merge completed work back to main

### **Experimenter Commands**
```bash
cd /home/ivan/projects/spawn-experiments
# Say "spawn-experiments" to Claude
# Provide APPLICATION_TYPE and TECH_STACK
# Review and approve generated baseline
# Launch four parallel method executions
# Compare results and integrate to main
```

---

## ðŸ”® Research Applications

### **For Development Teams**
- **Evidence-based methodology selection** for defined project scopes
- **Process optimization** based on quantified outcomes
- **Quality prediction** through methodology characteristics

### **For Researchers**
- **Methodology effectiveness measurement** with controlled scope
- **Baseline specification optimization** studies
- **Cross-domain validation** of methodology patterns

---

*Version 4.0 transforms spawn-experiments from pathology study to practical methodology optimization, enabling teams to make evidence-based decisions about AI collaboration approaches.*