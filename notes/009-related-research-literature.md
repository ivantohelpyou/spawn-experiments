# 009 - Related Research Literature - AI-Assisted Development Methodology Science

**Date**: September 20, 2025

**Context**: Identifying academic and industry research communities studying AI-assisted development patterns

## Core Research Domains

### 1. Empirical Software Engineering + AI Code Generation

#### Key Researchers & Institutions

**Prem Devanbu** (UC Davis)
- Pioneer in empirical software engineering and code naturalness
- Recent work on large language models for code
- Key papers: "On the Naturalness of Software" (foundational work on statistical properties of code)

**Baishakhi Ray** (Columbia University)
- Software engineering with machine learning focus
- Studies on code quality, testing, and AI-assisted development
- Research group: Software Systems Lab

**Dawn Song** (UC Berkeley)
- Security and AI intersection, code generation safety
- SkyPilot and other AI infrastructure projects
- Relevant for methodology validation and safety

**Satish Chandra** (Facebook/Meta)
- Static analysis, code understanding, AI-assisted development
- Industrial research perspective on AI coding tools

**Earl Barr** (University College London)
- Software testing, program analysis, developer tools
- Work on automated test generation relevant to your Method 4 validation

#### Relevant Conferences & Venues
- **ICSE** (International Conference on Software Engineering) - Premier venue
- **FSE** (Foundations of Software Engineering) - Empirical SE focus
- **ASE** (Automated Software Engineering) - AI + SE intersection
- **MSR** (Mining Software Repositories) - Data-driven SE research
- **ISSTA** (International Symposium on Software Testing and Analysis)

### 2. Human-Computer Interaction for Programming

#### Key Researchers

**Andrew Head** (University of Pennsylvania)
- Programmer behavior studies, IDE design, collaborative coding
- Relevant for understanding how developers interact with AI tools

**Brad Myers** (Carnegie Mellon University)
- End-user programming, developer tools, programming methodologies
- Long history of studying programming approaches

**Amy Ko** (University of Washington)
- Computing education, programming psychology, developer experience
- "The Design of Everyday Programming" research

**Emery Berger** (University of Massachusetts Amherst)
- Programming systems, performance analysis
- Might be interested in methodology performance comparisons

#### Relevant Venues
- **CHI** (Computer-Human Interaction) - Programmer behavior studies
- **VL/HCC** (Visual Languages and Human-Centric Computing)
- **ICER** (International Computing Education Research)

### 3. Software Engineering Methodology Research

#### Classic Methodology Researchers

**Alistair Cockburn**
- Agile methodologies, software development practices
- "Crystal Clear" and other methodology frameworks
- Industry perspective but methodology-focused

**Kent Beck**
- Test-Driven Development originator
- Extreme Programming methodology
- Would likely be very interested in empirical TDD validation

**Robert "Uncle Bob" Martin**
- Clean Code, SOLID principles
- Strong opinions on development practices that could be empirically tested

**Martin Fowler**
- Refactoring, software design patterns
- ThoughtWorks industrial research perspective

#### Academic Methodology Researchers

**Laurie Williams** (North Carolina State University)
- Empirical studies of pair programming, TDD, agile practices
- Has done controlled experiments on development methodologies

**Gail Murphy** (University of British Columbia)
- Software evolution, developer tools, program comprehension
- Relevant for component discovery research

**Mei Nagappan** (University of Waterloo)
- Empirical software engineering, mining software repositories
- Could provide methodological guidance for experiments

### 4. AI Code Generation Specific Research

#### Recent Pioneers

**Denny Zhou** (Google DeepMind)
- Chain-of-thought reasoning, AI reasoning patterns
- Relevant for understanding how prompting affects code generation

**Shunyu Yao** (Princeton/OpenAI)
- Tree of Thoughts, AI reasoning methodologies
- Highly relevant for systematic AI programming approaches

**Jin-Ge Yao** (Microsoft Research)
- Code generation, program synthesis
- IntelliCode and related AI-assisted development tools

**Marc Brockschmidt** (Microsoft Research)
- Program synthesis, neural code generation
- Graph neural networks for code

**Li Erran Li** (Amazon/Scale AI)
- AI systems, large language models for code
- Industrial research perspective

#### Industry Research Groups

**GitHub Next** (GitHub/Microsoft)
- Copilot research, AI-assisted development patterns
- Real-world deployment experience

**Google Research** - Programming Languages and Systems Group
- Code generation, program synthesis
- Large-scale empirical studies

**Facebook/Meta AI** - Code Intelligence
- AI-assisted development, automated testing
- Production deployment insights

**DeepMind** - Program Synthesis Team
- Formal verification, mathematical reasoning in code
- Relevant for test validation methodology

### 5. Software Testing & Validation Research

#### Key Researchers

**Andreas Zeller** (CISPA Helmholtz Center)
- Automated debugging, test generation, software analysis
- "The Fuzzing Book" - highly relevant for test validation

**Gordon Fraser** (University of Passau)
- Automated test generation, mutation testing
- EvoSuite tool - relevant for test quality validation

**Sarfraz Khurshid** (University of Texas at Austin)
- Software testing, constraint solving, program analysis
- Alloy and other formal methods

**Jeff Offutt** (George Mason University)
- Software testing, test criteria, mutation testing
- Theoretical foundations for test quality measurement

#### Testing & Validation Venues
- **ICST** (International Conference on Software Testing)
- **ISSTA** (International Symposium on Software Testing and Analysis)
- **FASE** (Fundamental Approaches to Software Engineering)

### 6. Empirical Studies on Programming Practices

#### Recent Relevant Studies

**"An Empirical Study of Programming Language Trends"** (Meyerovich & Rabkin, 2013)
- Large-scale studies of developer practices
- Methodology for empirical programming research

**"A Large-Scale Study of Programming Languages and Code Quality in GitHub"** (Ray et al., 2014)
- Empirical analysis of coding practices
- Relevant methodology for large-scale code analysis

**"The Impact of Test-Driven Development on Software Development Productivity"** (Various researchers)
- Multiple studies with conflicting results
- Your research could provide clearer evidence

#### Ongoing Research Programs

**PROMISE** (Predictor Models in Software Engineering)
- International community doing empirical SE research
- Annual conference and shared datasets

**ReplicationOrg**
- Focused on replicating empirical software engineering studies
- Could help validate your findings

### 7. Programming Psychology & Cognitive Science

#### Key Researchers

**Antti-Pekka Tuovinen** (University of Helsinki)
- Cognitive load in programming, learning programming
- Relevant for understanding methodology cognitive overhead

**Felienne Hermans** (Leiden University)
- Programming education, cognitive aspects of programming
- "The Programmer's Brain" research

**Janet Siegmund** (University of Passau)
- Program comprehension, cognitive aspects of software development
- Empirical studies using neuroscience methods

## Potential Collaboration Opportunities

### Academic Collaboration Strategies

#### Conference Paper Submissions
1. **ICSE 2026** (Research Track) - Full empirical study
2. **FSE 2025** (Industry Track) - Practical implications
3. **ASE 2025** (Tool Demo) - Framework for methodology comparison
4. **MSR 2025** (Data Track) - Open dataset of methodology comparisons

#### Workshop Papers & Positioning
1. **WAMA** (Workshop on AI for Software Engineering)
2. **SPLASH** (Dynamic Languages Symposium)
3. **Empirical Software Engineering** journal special issues

#### Open Science Approach
1. **Publish raw data** on methodology comparisons
2. **Open source framework** for replication
3. **Preprint servers** (arXiv cs.SE) for early feedback
4. **Blog series** targeting practitioner communities

### Industry Collaboration Opportunities

#### AI Coding Tool Companies
1. **GitHub** (Copilot team) - Real-world validation of methodology patterns
2. **Anthropic** (Claude team) - Model-specific methodology optimization
3. **Cursor** (AI editor) - IDE integration insights
4. **Replit** (AI coding) - Educational applications

#### Research Labs
1. **Microsoft Research** - Large-scale empirical studies
2. **Google Research** - Production system validation
3. **Meta AI** - Social coding aspects
4. **DeepMind** - Formal verification connections

### Community Building Strategies

#### Online Presence
1. **Twitter/X academic community** - Share findings, connect with researchers
2. **Reddit r/MachineLearning, r/programming** - Practitioner feedback
3. **Academic blogs** (Blogforcse.org, etc.) - Broader reach
4. **YouTube/talks** - Conference presentations and explanations

#### Professional Networks
1. **ACM SIGSOFT** - Software engineering community
2. **IEEE Computer Society** - Broader computing research
3. **SIGCHI** - Human-computer interaction aspects
4. **Local university connections** - Guest lectures, collaborations

## Specific Paper Recommendations for Deep Reading

### Foundational Empirical SE
1. **"On the Naturalness of Software"** (Hindle et al., 2012)
2. **"A Large Scale Study of Programming Languages and Code Quality"** (Ray et al., 2014)
3. **"The Unreasonable Effectiveness of Data in Empirical Software Engineering"** (Menzies et al., 2013)

### AI Code Generation
1. **"Evaluating Large Language Models Trained on Code"** (Chen et al., 2021)
2. **"Competition-level code generation with AlphaCode"** (Li et al., 2022)
3. **"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models"** (Wang et al., 2021)

### Methodology Studies
1. **"Realizing quality improvement through test driven development"** (Nagappan et al., 2008)
2. **"A comparative case study on the impact of test-driven development"** (Erdogmus et al., 2005)
3. **"Test-driven development as a defect-reduction practice"** (Williams et al., 2003)

### Programming Behavior
1. **"The Design of Everyday Programming"** (Ko et al., 2011)
2. **"Programming, Problem Solving, and Self-Awareness"** (Prather et al., 2020)
3. **"Understanding Programming Expertise"** (Adelson & Soloway, 1985)

## Positioning Your Research

### Unique Contributions
1. **First systematic empirical study** of AI-assisted development methodologies
2. **Controlled experimental approach** vs. observational studies
3. **Multi-tier complexity progression** (function → tool → application)
4. **Component discovery research** - novel angle on reuse patterns
5. **Open replication framework** - enables community validation

### Research Gap Identification
1. **Most AI code research focuses on models, not methodologies**
2. **TDD research predates AI assistance - needs updating**
3. **No systematic component discovery studies in AI context**
4. **Limited empirical validation of AI coding best practices**

### Community Value Proposition
1. **Evidence-based methodology selection** vs. philosophical arguments
2. **Practical guidance for teams** adopting AI coding tools
3. **Benchmark framework** for evaluating new AI coding approaches
4. **Open dataset** for future methodology research

## Next Steps for Academic Engagement

### Short Term (1-3 months)
1. **Literature review paper** - Position your work in academic context
2. **Workshop submission** - Present preliminary findings
3. **Reach out to 2-3 key researchers** - Share early results, seek feedback
4. **Join academic Twitter** - Engage with SE research community

### Medium Term (3-6 months)
1. **Conference paper submission** - Full empirical study
2. **Industry collaboration** - Validate findings with AI tool companies
3. **Dataset publication** - Open science contribution
4. **Blog series** - Build practitioner audience

### Long Term (6-12 months)
1. **Journal paper** - Comprehensive methodology science framework
2. **Tool publication** - Replication framework for community
3. **PhD program consideration** - If interested in deep academic career
4. **Industry research position** - Alternative path with companies

This research is positioned at the intersection of several hot academic areas and has strong potential for both academic impact and practical value.

---

*Note: This literature review identifies clear positioning for spawn-experiments research at the intersection of empirical software engineering, AI code generation, and programming methodology science. The work fills a significant gap in systematic empirical evaluation of AI-assisted development practices.*