# Future Experiments Roadmap - Hierarchical System (T.DCC)

> **LATEST**: Experiment 010-password-generator completed with 4-method comparison
> **Results**: Method timing validated - 1m17s (immediate) to 9m20s (validated TDD)

> **NEW**: Reorganized with hierarchical numbering system (T.DCC format)
> See [EXPERIMENT_NUMBERING_SYSTEM.md](EXPERIMENT_NUMBERING_SYSTEM.md) for complete mapping

## Current Status

### Completed Experiments (Versioned Numbering)
1. **1.201.0** - Expression Evaluator (Math/Parsing) - 35 min total
2. **1.203.0** - Simple Interest Calculator (Basic Math/CLI) - Smoke test
3. **1.301** - LRU Cache with TTL (Data Structures) - ❌ **STOPPED** - Methods 3&4 hit token limits, incomplete
4. **1.302.0** - LRU Cache with TTL (Data Structures/Performance) - ✅ **COMPLETE** - 6-13 min per method, successful parallel execution
5. **1.102.0** - Multilingual Word Counter (String Processing) - ✅ **COMPLETE** - validation methodology successful
6. **1.401.0** - Password Generator (Security/Crypto) - ✅ **COMPLETE** - Tier 1 validation
7. **1.205.0** - Prime Number Generator (Mathematical) - ✅ **COMPLETE** - methodology comparison successful
8. **1.101.0** - Anagram Grouper (String Processing) - ✅ **COMPLETE** - TDD won with 3X less code
9. **1.103.0** - Roman Numeral Converter (String Processing) - ✅ **COMPLETE** - fastest TDD at 3m 37s
10. **1.104.0** - Balanced Parentheses (String Processing) - ✅ **COMPLETE** - Tier 1 series concluded
11. **1.501** - Email Validator (Input Validation) - ✅ **COMPLETE** - 3.6X code reduction with TDD
12. **1.502** - URL Validator (Input Validation) - ✅ **COMPLETE** - 32.3X over-engineering discovered!
13. **1.503** - File Path Validator + Competition Injection - ✅ **COMPLETE** - 🚨 **BREAKTHROUGH: Reversible over-engineering proven!**
14. **1.504** - Date Format Validator + V4 Framework Enhancement - ✅ **COMPLETE** - 🔮 **BREAKTHROUGH: AI bias detection via prediction accountability!**

*Legacy numbers: 002→1.201.0, 006→1.203.0, 007→1.301(STOPPED), 008→1.302.0, 009→1.102.0, 010→1.401.0, 011→1.205.0, 012→1.101.0, 013→1.103.0, 014→1.104.0*

### Key Insights Discovered
- **🔍 LATEST: Component Discovery Needs Awareness**: Agents require explicit hints about available components (2.505/2.505.1)
- **⚖️ LATEST: External Library Trade-off**: 70% time increase for richer features, challenges speed assumptions (Method 1E)
- **🎯 LATEST: Integration Pattern Consistency**: Methodologies integrate components in characteristic ways (fallback, registry, direct, strategic)
- **🔮 NEW: V4 Framework Enhancement Success**: Enhanced framework with prediction accountability reveals AI biases
- **🏆 NEW: Method 4 V4.1 Adaptive TDD Breakthrough**: Optimal balance (4m dev, 1M+ val/sec, strategic validation)
- **📊 NEW: AI Bias Detection via Predictions**: Systematic underestimation of simple approaches, overestimation of specification complexity
- **⚡ NEW: Performance Optimization Discovery**: V4.1 achieved 80% faster runtime than immediate implementation
- **🚨 BREAKTHROUGH: Competition Injection Reverses Over-Engineering**: File Path (4,530→687 lines with constrained pressure)
- **Escalating Over-Engineering Pattern**: URL (32.3X) > File Path (22.1X) > Date (6.4X) > Email (3.6X) complexity explosion
- **Enterprise AI Development Achievable**: Constrained interventions maintain both speed and documentation quality
- **AI Adaptability Under Pressure**: Proven ability to right-size methodology application while preserving integrity
- **Time Convergence**: Most methods complete in 8-14 minutes regardless of approach
- **Parallel Launch Success**: Simultaneous execution proven feasible (009)
- **Resource Constraints Impact**: Token limits significantly affected Methods 3&4 in 007
- **TDD Efficiency on Simple Problems**: Method 3 produces clean, efficient code for Tier 1 algorithms
- **Method 4 Evolution**: V4.0 had incomplete implementations, V4.1 Adaptive TDD achieves optimal results
- **Specification Explosion Risk**: Method 2 over-engineers simple problems (URL: 6,036 vs 187 lines!)
- **Test Validation Innovation**: V4.1's adaptive validation methodology provides strategic quality assurance
- **Methodology Selection Context-Dependent**: Different approaches optimal for different complexity tiers
- **AI Resource Usage Patterns**: TDD approaches require more tool interactions (54 vs 29 in 007)
- **V4 Framework Evolution**: Enhanced from rigid guided TDD to adaptive complexity-matching approach

## New Experimental Framework: Three-Tier System

Based on lessons learned, future experiments follow a **crawl-walk-run** progression:

### **Tier 1: Functions (010-019) - CRAWL**
**Scope**: Pure algorithmic problems, single functions, stdlib only
**Duration**: 5-15 minutes per approach (30-60 min total parallel)
**Claude Code Usage**: ✅ **Safe for any usage window**
**Purpose**: Isolate methodology differences without architectural complexity
**Status**: ✅ **COMPLETE** - Series 010-014 finished

#### Completed Tier 1 Experiments
- **1.401.0 - Password Generator**: ✅ Cryptographic randomness, character set manipulation
- **1.205.0 - Prime Number Generator**: ✅ Algorithm choice and optimization
- **1.101.0 - Anagram Grouper**: ✅ Hash key strategy and grouping logic - **TDD Winner**
- **1.103.0 - Roman Numeral Converter**: ✅ Mapping strategy and edge cases - **Fastest TDD**
- **1.104.0 - Balanced Parentheses**: ✅ Stack management and character matching

#### Tier 1 Extensions - Input Validation Domain (1.5XX)
**Purpose**: Bridge gap between pure algorithms and CLI tools for better composability

##### ✅ **Completed Input Validation Experiments**
- **1.501 - Email Validator**: ✅ **COMPLETE** - Revolutionary 3.6X code reduction finding, security vulnerability analysis
- **1.502 - URL Validator**: ✅ **COMPLETE** - **32.3X OVER-ENGINEERING!** Largest complexity explosion documented

##### ✅ **ALL Input Validation Experiments COMPLETE**
- **1.501 - Email Validator**: ✅ **COMPLETE** - 3.6X complexity explosion baseline
- **1.502 - URL Validator**: ✅ **COMPLETE** - 32.3X complexity explosion peak
- **1.503 - File Path Validator**: ✅ **COMPLETE** - 7.4X complexity with competition injection breakthrough
- **1.504 - Date Format Validator**: ✅ **COMPLETE** - 6.4X complexity with V4 framework enhancement

##### ✅ **Completed Input Validation Experiments**
- **1.501 - Email Validator**: ✅ **COMPLETE** - 3.6X complexity explosion baseline
- **1.502 - URL Validator**: ✅ **COMPLETE** - 32.3X complexity explosion peak
- **1.503 - File Path Validator**: ✅ **COMPLETE** - 7.4X complexity with competition injection breakthrough
- **1.504 - Date Format Validator**: ✅ **COMPLETE** - 6.4X complexity with V4 framework enhancement
- **1.505 - JSON Schema Validator**: ✅ **COMPLETE** - 3.1X complexity, V4.1 confirmed optimal
- **⚠️ 1.501.1 - Email Validator Severed Branch**: ✅ **COMPLETE** - DATA GAP: Universal 3-4x acceleration proven but compared against complexity metrics, not timing
- **2.505 - JSON Schema Validator CLI**: ✅ **COMPLETE** - 0% component discovery baseline established
- **2.505.1 - Guided Component Discovery + External Libraries**: ✅ **COMPLETE** - BREAKTHROUGH: 100% discovery success, comprehensive 8-method study (Methods 1/2/3/4 + 1E/2E/3E/4E external variants)
- **2.505.2 - Severed Branch Timing**: ✅ **COMPLETE** - Complex multi-variable design with component discovery + context isolation

##### 📋 **Optional Tier 1A Extensions**
- **1.506 - IPv4/IPv6 Address Validator**: Network address validation *(OPTIONAL - pattern established)*
- **1.507 - Regular Expression Validator**: Pattern validation with safety checking *(OPTIONAL)*

**Tier 1B: Regulatory/Infrastructure Dependencies**
- **1.508 - Phone Number Validator**: International formats *(LOWER PRIORITY - regulatory complexity)*
- **1.509 - Credit Card Number Validator**: Financial patterns *(LOWER PRIORITY - payment gateway dependency)*
- **1.510 - Tax ID Validator**: Government ID formats *(LOWER PRIORITY - regulatory complexity)*

**Research Note**: Tier 1A functions are composable and deployable without additional infrastructure. Tier 1B functions require regulatory compliance and external systems for practical deployment, creating 2^n complexity scaling.

**Pattern CONFIRMED**: Consistent over-engineering in Method 2 (avg 12.4X across 4 experiments), V4.1 Adaptive TDD emerges as optimal approach
**NEW PATTERN**: Component discovery requires awareness (agents don't scan codebases by default), external libraries provide features at 70% time cost

### **Tier 2: CLI Tools (2.XXX) - WALK**
**Scope**: Command-line utilities, file I/O, composable tools
**Duration**: 15-30 minutes per approach (60-120 min total parallel)
**Claude Code Usage**: ⚠️ **Requires >2 hours remaining** (based on 007 resource analysis)
**Purpose**: Study interface design and component composition
**Components Available**: Best-of-breed validators in utils/validation/
**Innovation**: Component discovery research - natural reuse vs. rebuild patterns

#### ✅ **Completed Tier 2 Experiments**
- **2.505 - JSON Schema Validator CLI**: ✅ **COMPLETE** - 0% component discovery across all methods (baseline)
- **2.505.1 - Guided Component Discovery**: ✅ **COMPLETE** - 100% discovery with awareness hint + comprehensive 8-method external library analysis (all methodologies tested with internal and external variants)

#### 🚀 **High-Priority Tier 2 Experiments**
- **2.506 - Configuration File Parser**: Validate component discovery across domains (YAML/JSON/INI)
- **2.507 - Cross-Domain Component Reuse**: Test discovery patterns across different problem domains
- **2.508 - Component Library Scaling**: Test with larger component libraries and complex integration scenarios

#### Planned Tier 2 Experiments (Strategic Component Alignment)
- **2.501 - Password Manager CLI**: Natural reuse of 1.401.0 password generation
- **2.201 - Number Theory Calculator**: Strategic reuse of 1.205.0 primes, 1.103.0 numerals
- **2.102 - Text Analysis Tool**: Natural reuse of 1.101.0 anagram grouping
- **2.202 - Code Structure Validator**: Strategic reuse of 1.104.0 parentheses matching
- **2.401 - File Statistics Tool**: Pure CLI tool with minimal component dependencies (baseline)
- **2.507 - Cross-Domain Component Reuse**: Components across problem domains

#### Component Discovery Research Framework ✅ **ESTABLISHED**
**Key Finding**: Agents need explicit awareness of available components (sensible default behavior)
**Patterns Identified**: Each methodology integrates components consistently with their approach
**External Library Insight**: 70% time increase for richer features (Method 1E: 4m 41.5s vs Method 1: 2m 45.1s)
**Next Research**: Methodology-specific external library integration patterns and constraints

### **Tier 3: Applications (030-039) - RUN**
**Scope**: Full applications with GUIs, APIs, persistence
**Duration**: 45-90 minutes per approach (180-360 min total parallel)
**Claude Code Usage**: ❌ **Requires >4 hours remaining** (based on 007 token analysis)
**Purpose**: Study complex system architecture and integration
**Components Available**: Functions (Tier 1) + Tools (Tier 2) discoverable
**Resource Planning**: Account for higher tool usage in Methods 3&4 (up to 54 interactions vs 29)

#### Planned Tier 3 Experiments
- **3.101 - Personal Knowledge Manager**: Note-taking with search and tagging
- **3.201 - Project Dashboard**: Development metrics and build monitoring
- **3.401 - Personal Finance Tracker**: Expense tracking with budgets and reports
- **3.301 - System Monitor**: Resource monitoring with alerts and history
- **3.102 - Document Processor**: Batch format conversion and workflow automation

## Discovered Components Research

### Key Innovation
Study **organic component discovery** patterns without explicit guidance:
- Components placed in `./utils/functions/` and `./utils/tools/`
- No mention in experiment prompts
- Natural exploration and reuse decisions measured
- Authentic development environment simulation

### Research Questions
1. **Discovery Patterns**: Which methodologies naturally explore existing codebases?
2. **Evaluation Criteria**: How do approaches assess found components for fitness?
3. **Integration Strategies**: What drives reuse vs. rebuild decisions?
4. **Architecture Influence**: How does component availability affect system design?

## Experimental Improvements

### Bias Prevention
- **Neutral naming enforced**: 1-immediate-implementation, 2-specification-driven, etc.
- **Language monitoring**: No quality indicators (naive/advanced/optimal)
- **Pre-experiment confirmation**: User validates bias-neutral setup
- **Post-experiment reporting**: Standard testing instructions with warnings

### Quality Improvements
- **Testing warnings**: Flag slow dependencies (pandas, heavy ML libs)
- **Quick test paths**: Always identify fast validation approach
- **Protocol compliance**: Transparent documentation of any violations
- **Reproducibility**: Clear instructions for result validation

## Critical Data Gap - Severed Branch Timing Study

### ⚠️ **URGENT: Proper Severed Branch Validation Required**

**Problem Identified**: Experiment 1.501.1 successfully validated severed branch acceleration hypothesis (3-4x universal speedup) but compared against 1.501's complexity metrics instead of precise timing data. We need clean timing comparison.

**Required Experiment**: **X.X.X - Severed Branch vs Normal Timing Study**
- **Scope**: Use experiment with FULL TIMING DATA (e.g., 1.507.3, 2.505.1, or 2.505.2)
- **Method**: Run same experiment both ways: normal context vs severed branch isolation
- **Variable**: Single variable - development context isolation only
- **Baseline**: Must have precise timing measurements for direct comparison
- **Priority**: HIGH - validates core acceleration technique

**Candidate Experiments for Timing Comparison**:
1. **1.507.3 QR Code Generator** - Has precise timing logs for all 4 methods
2. **2.505.1 Guided Component Discovery** - Has timing data + component discovery patterns
3. **2.505.2 Severed Branch Complex** - Already has severed branch data, run normal context version

**Research Question**: Does severed branch isolation maintain 3-4x acceleration when compared against actual timing data (not complexity metrics)?

### Priority Execution Plan

### Phase 0: DATA GAP RESOLUTION (IMMEDIATE)
**CRITICAL**: Execute proper severed branch timing validation using experiment with existing precise timing data.

### Phase 1: Tier 1 Foundation (Next 2-3 months)
Execute experiments 010-014 to build function library and establish baseline methodology patterns.

### Phase 2: Tier 2 Composition (Following 2-3 months)
Execute experiments 020-024 to study tool composition and interface design with available function components.

### Phase 3: Tier 3 Integration (Final phase)
Execute experiments 030-034 to analyze complex system architecture with full component ecosystem.

## Success Metrics

### Quantitative
- **Component reuse rates** across methodologies and tiers
- **Development speed** with and without available components
- **Quality metrics** (correctness, test coverage, maintainability)
- **Architecture complexity** measures

### Qualitative
- **Natural discovery patterns** for existing components
- **Integration strategy differences** between methodologies
- **Methodology consistency** across complexity tiers
- **Realistic development behavior** patterns

## Expected Outcomes

1. **Methodology Scaling**: How approaches adapt across complexity levels
2. **Component Utilization**: Which methodologies naturally leverage existing work
3. **Architecture Emergence**: What design patterns emerge from generative development
4. **Development Efficiency**: Cumulative benefits of building block availability

This tiered approach addresses the scope ambiguity problem while providing unprecedented insight into realistic development scenarios with existing codebases.

## Future Organizational Improvements

### Hierarchical Numbering System (Proposed)
**Problem**: Current sequential numbering (010, 011, 012...) requires renumbering when inserting experiments
**Solution**: Dewey Decimal-inspired system allowing infinite extensions without renumbering

#### Proposed Structure
- **Tier 1 Functions**: 1.001, 1.002, 1.003, etc.
- **Tier 2 CLI Tools**: 2.001, 2.002, 2.003, etc.
- **Tier 3 Applications**: 3.001, 3.002, 3.003, etc.
- **Special Studies**: 4.001 (methodology comparisons), 5.001 (replication studies), etc.

#### Benefits
- **Extensible**: Insert 1.015 between 1.001 and 1.002 if needed
- **Categorical**: Tier immediately visible from number
- **Future-proof**: Supports sub-categories (1.001.1, 1.001.2 for variations)
- **Legacy Mapping**: Current 010-014 → 1.001-1.005

#### Implementation Status
- **Phase 1**: ✅ **COMPLETE** - All documentation updated with new numbering system
- **Phase 2**: 🔄 **OPTIONAL** - Directory symlinks for backward compatibility (future)
- **Phase 3**: ✅ **ACTIVE** - All future experiments use new numbering format
- **Phase 4**: 📚 **FUTURE** - Physical directory migration (low priority)

#### Backward Compatibility Guide
**All legacy experiment references remain valid:**
- Directory names unchanged (experiments/012-anagram-grouper/)
- Legacy numbers in documentation show mapping (012 → 1.101)
- GitHub links and bookmarks continue working
- Gradual transition - no breaking changes

**Quick Reference Card:**
```
002 → 1.201.0 | 006 → 1.203.0 | 007 → 1.301(STOPPED) | 008 → 1.302.0 | 009 → 1.102.0
010 → 1.401.0 | 011 → 1.205.0 | 012 → 1.101.0 | 013 → 1.103.0 | 014 → 1.104.0
```