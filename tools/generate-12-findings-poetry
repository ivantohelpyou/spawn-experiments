#!/usr/bin/env python3
"""
Generate poetry for all 12 findings and concatenate to a single file.
"""

import subprocess
import time
from pathlib import Path
from datetime import datetime

# 12 Research summaries based on findings
FINDINGS = [
    {
        "number": "01",
        "title": "AI Over-Engineering Patterns",
        "summary": "AI code generators exhibit systematic over-engineering when given vague requirements. Without constraints, they create enterprise-grade solutions for simple tasks, adding unnecessary complexity like rate limiting and batch processing for basic validators."
    },
    {
        "number": "02",
        "title": "Architectural Convergence Patterns",
        "summary": "Despite starting with different methodologies, AI implementations converge toward similar architectural patterns. Class-based designs with clear separation of concerns emerge naturally, suggesting certain code structures are inherently more discoverable."
    },
    {
        "number": "03",
        "title": "Complexity Matching Principle",
        "summary": "Effective AI-assisted development requires matching methodology complexity to task complexity. Over-engineered approaches waste time on simple tasks, while under-engineered approaches fail on complex systems. The right methodology depends entirely on problem characteristics."
    },
    {
        "number": "04",
        "title": "Component Discovery Breakthrough",
        "summary": "Using external code components as references dramatically accelerates AI development. When shown existing implementations, AI tools understand requirements faster and produce better results. Component-guided development reduces iteration cycles significantly."
    },
    {
        "number": "05",
        "title": "External Library Efficiency",
        "summary": "External libraries outperform AI-generated implementations by ten to one thousand times. For well-solved problems like date validation, using established libraries provides better performance, fewer bugs, and less maintenance burden than custom AI-generated code."
    },
    {
        "number": "06",
        "title": "External vs Internal Components",
        "summary": "The choice between external libraries and internal implementations depends on problem novelty. Use external libraries for solved problems with mature ecosystems. Build internally for novel problems, domain-specific logic, or when dependencies create more complexity than value."
    },
    {
        "number": "07",
        "title": "Input Validation Patterns",
        "summary": "AI-generated input validation follows predictable patterns across methodologies. All approaches create similar error detection logic, but differ in code organization and test coverage. The validation logic itself remains remarkably consistent."
    },
    {
        "number": "08",
        "title": "Selective TDD Discovery",
        "summary": "Strategic test-driven development outperforms uniform approaches. Adaptive TDD focuses testing effort on complex areas while keeping simple code lightweight. This accidental discovery proved more effective than rigidly applying TDD everywhere."
    },
    {
        "number": "09",
        "title": "Prompt Engineering Force Multiplier",
        "summary": "Optimized prompts improve both speed and quality across all methodologies. Better prompts reduced generation time by twenty-two to thirty-six percent while simultaneously improving output quality. Prompt engineering amplifies every development approach."
    },
    {
        "number": "10",
        "title": "Monte Carlo Methodology Sampling",
        "summary": "Generating multiple samples and selecting the best produces twenty percent quality improvement. This Monte Carlo approach works across all methodologies, making it a practical production technique for critical code where quality matters more than development speed."
    },
    {
        "number": "11",
        "title": "LLM Integration Advantage",
        "summary": "Specification-driven development consistently wins for LLM integration projects. Method two averages ninety-two out of one hundred quality score, outperforming faster methods by ten to eighteen points. Complex systems with external dependencies benefit from upfront design."
    },
    {
        "number": "12",
        "title": "Problem-Type Performance Variance",
        "summary": "No single methodology wins across all problem types. Specification-driven excels at LLM integration but creates thirty-two times code bloat on simple validators. Methodology performance depends on problem complexity profile, not task difficulty alone."
    }
]

def main():
    tool_path = Path(__file__).parent / "generate-poetry"
    output_file = Path(__file__).parent.parent / "notes" / "12-findings-poetry-showcase.md"

    # Create header
    content = f"""# 12 Findings Poetry Showcase
## All Research Findings as Poetry

Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

Each of the 12 validated findings from spawn-experiments converted into 3 poetry formats:
- ðŸŽ‹ **Haiku** (5-7-5 syllables)
- ðŸ“œ **Iambic Pentameter** (10 syllables/line)
- ðŸŽª **Limerick** (AABBA rhyme scheme)

All outputs are **gold medal winners** (Method 2: Specification-Driven).

---

"""

    # Generate poetry for each finding
    for i, finding in enumerate(FINDINGS, 1):
        print(f"[{i}/12] Processing: {finding['title']}...")

        # Add finding header
        content += f"""## Finding {finding['number']}: {finding['title']}

**Research Summary:**
> {finding['summary']}

**[â†’ Read full finding](https://github.com/ivantohelpyou/spawn-experiments/blob/main/findings/{finding['number']}-{finding['title'].lower().replace(' ', '-').replace(':', '')}.md)**

"""

        # Run generate-poetry
        try:
            result = subprocess.run(
                [str(tool_path), finding['summary']],
                capture_output=True,
                text=True,
                timeout=180,
                cwd=tool_path.parent
            )

            if result.returncode == 0:
                # Extract just the gold medal results section
                output = result.stdout
                start_marker = "ðŸ¥‡ GOLD MEDAL RESULTS"
                end_marker = "âœ¨ Generated"

                start_idx = output.find(start_marker)
                end_idx = output.find(end_marker)

                if start_idx != -1 and end_idx != -1:
                    poetry_section = output[start_idx:end_idx].strip()
                    content += poetry_section + "\n\n---\n\n"
                else:
                    content += "*[Could not parse poetry output]*\n\n---\n\n"

                print(f"    âœ“ Generated")
            else:
                content += "*[Generation failed]*\n\n---\n\n"
                print(f"    âœ— Failed")

        except subprocess.TimeoutExpired:
            content += "*[Timeout - could not generate poetry]*\n\n---\n\n"
            print(f"    âœ— Timeout")
        except Exception as e:
            content += f"*[Error: {e}]*\n\n---\n\n"
            print(f"    âœ— Error: {e}")

        # Wait 2 seconds between runs (except last one)
        if i < len(FINDINGS):
            time.sleep(2)

    # Add footer
    content += f"""
## Technical Details

- **Total Findings**: 12
- **Total Poems**: 36 (12 findings Ã— 3 formats)
- **Method**: Specification-Driven (Method 2) - Gold medal winner
- **LLM Model**: llama3.2 (poetry generation)
- **Evaluation**: Claude Sonnet 4.5 (code quality assessment)
- **Generated**: {datetime.now().strftime('%Y-%m-%d')}

---

*Generated by: tools/generate-12-findings-poetry*
"""

    # Write to file
    output_file.write_text(content)
    print(f"\nâœ“ Complete! Output written to: {output_file}")
    print(f"âœ“ Total: 12 findings Ã— 3 poetry formats = 36 poems")

if __name__ == "__main__":
    main()
